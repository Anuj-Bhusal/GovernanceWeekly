try:
    from readability import Document
except ImportError:
    Document = None
from bs4 import BeautifulSoup
import dateparser
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

def extract_article(html, url):
    """
    Extracts the main content, title, and metadata from an article HTML.
    """
    if not html:
        return None
        
    try:
        if Document:
            doc = Document(html)
            title = doc.short_title()
            content_html = doc.summary()
        else:
            # Fallback for missing readability
            soup = BeautifulSoup(html, "html.parser")
            title = soup.title.string if soup.title else "No Title"
            # Try to find <article> or just body
            article_body = soup.find('article') or soup.body
            content_html = str(article_body) if article_body else html
        
        # Clean up tags using BeautifulSoup
        soup = BeautifulSoup(content_html, "html.parser")
        text_content = soup.get_text(separator="\n").strip()
        
        # Remove junk patterns (read time, date stamps, social media prompts, etc.)
        import re
        junk_patterns = [
            r'Read Time\s*:.*?\d+\s*(?:minute|min|second|sec)',  # Read Time : < 1 minute
            r'Read Time\s*:.*?2o?\d{2}\s+\w+\s+\d+\s+\w+\s+o?\d{1,2}:o?\d{2}:o?\d{2}',  # Read Time : > 3 minutes 2o82 Paush 3 Thursday o6:36:oo
            r'2o?\d{2}\s+(?:Paush|Magh|Falgun|Chaitra|Baisakh|Jestha|Ashadh|Shrawan|Bhadra|Ashwin|Kartik|Mangsir)\s+\d+\s+(?:Sunday|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday)\s+o?\d{1,2}:o?\d{2}:o?\d{2}',  # Nepali calendar dates
            r'\d{4}\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2}\s+(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\s+\d{1,2}:\d{2}',  # Date stamps
            r'News Summary Generated by OK AI\.?\s*Editorially reviewed\.?',  # OnlineKhabar AI summary notice
            r'Share on Facebook.*?Share on Twitter',  # Social media
            r'Share via Email',
            r'Print this page',
            r'Advertisement',
            r'Related Articles?:?',
            r'Tags?:.*',
            r'Published on:.*',
            r'Updated on:.*',
            r'\[.*?\]',  # [Photo Gallery] etc
        ]
        
        for pattern in junk_patterns:
            text_content = re.sub(pattern, '', text_content, flags=re.IGNORECASE)
        
        # Remove excessive whitespace
        text_content = re.sub(r'\n\s*\n+', '\n\n', text_content)
        text_content = text_content.strip()
        
        # Metadata extraction (heuristic)
        # Often readability doesn't get the date well, so we might need custom parsing per site
        # or use generic meta tag scraping
        
        soup_full = BeautifulSoup(html, "html.parser")
        
        # Attempt to find published time from multiple sources
        published_at = None
        
        # Try multiple date extraction methods
        date_candidates = []
        
        # 1. Meta tags
        for meta_tag in soup_full.find_all("meta"):
            for attr in ['property', 'name', 'itemprop']:
                if meta_tag.get(attr) in ['article:published_time', 'publishedDate', 'datePublished', 'date', 'publication_date']:
                    content = meta_tag.get("content")
                    if content:
                        date_candidates.append(content)
        
        # 2. Time tags
        for time_tag in soup_full.find_all("time"):
            datetime_attr = time_tag.get("datetime") or time_tag.get_text()
            if datetime_attr:
                date_candidates.append(datetime_attr)
        
        # 3. JSON-LD structured data
        for script in soup_full.find_all("script", type="application/ld+json"):
            try:
                import json
                data = json.loads(script.string)
                if isinstance(data, dict):
                    if 'datePublished' in data:
                        date_candidates.append(data['datePublished'])
                    elif 'dateCreated' in data:
                        date_candidates.append(data['dateCreated'])
            except:
                pass
        
        # 4. Common date class patterns
        for date_elem in soup_full.find_all(class_=re.compile(r'date|time|publish', re.I)):
            date_text = date_elem.get_text().strip()
            if date_text:
                date_candidates.append(date_text)
        
        # Parse first valid date
        for candidate in date_candidates:
            try:
                parsed = dateparser.parse(str(candidate), settings={'PREFER_DATES_FROM': 'past'})
                if parsed:
                    published_at = parsed
                    break
            except:
                continue
        
        return {
            "title": title,
            "full_text": text_content,
            "published_at": published_at,
            "raw_html": html
        }
        
    except Exception as e:
        logger.error(f"Extraction failed for {url}: {e}")
        return None
